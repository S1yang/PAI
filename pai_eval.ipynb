{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from attention import llama_modify\n",
    "from constants import INSTRUCTION_TEMPLATE, SYSTEM_MESSAGE\n",
    "from llava.utils import disable_torch_init\n",
    "from model_loader import ModelLoader\n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from PIL import Image\n",
    "\n",
    "class FileListCOCODataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, img_dir, trans):\n",
    "        with open(file_list, \"r\") as f:\n",
    "            self.img_names = [x.strip() for x in f if x.strip()]\n",
    "        self.img_dir = img_dir\n",
    "        self.trans = trans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_id = int(os.path.splitext(img_name)[0].split('_')[-1])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.trans:\n",
    "            image = self.trans(image)\n",
    "        return {\"img_id\": img_id, \"image\": image}\n",
    "\n",
    "\n",
    "# ========== 变量直接指定 ==========\n",
    "model_name = \"llava-1.5\"\n",
    "txt_file = \"../../auto_cir/selected_images-160.txt\"\n",
    "img_dir = \"../../img-set/val2014\"\n",
    "output_jsonl = \"pai_captions.jsonl\"\n",
    "gpu_id = \"0\"\n",
    "use_cfg = False\n",
    "use_attn = True   \n",
    "\n",
    "alpha = 0.5\n",
    "gamma = 1.2\n",
    "beam = 5\n",
    "sample = False\n",
    "batch_size = 1\n",
    "max_tokens = 128\n",
    "start_layer = 2\n",
    "end_layer = 32\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "disable_torch_init()\n",
    "\n",
    "# ========== 新增: 针对图片清单的 DataSet ==========\n",
    "class FileListCOCODataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, img_dir, trans):\n",
    "        with open(file_list, \"r\") as f:\n",
    "            self.img_names = [x.strip() for x in f if x.strip()]\n",
    "        self.img_dir = img_dir\n",
    "        self.trans = trans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_id = int(os.path.splitext(img_name)[0].split('_')[-1])\n",
    "        from PIL import Image\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.trans:\n",
    "            image = self.trans(image)\n",
    "        return {\"img_id\": img_id, \"image\": image}\n",
    "\n",
    "# ========== 加载模型与预处理 ==========\n",
    "model_loader = ModelLoader(model_name)\n",
    "dataset = FileListCOCODataSet(txt_file, img_dir, model_loader.image_processor)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# ========== Prompt 模板 ==========\n",
    "template = INSTRUCTION_TEMPLATE[model_name]\n",
    "if model_name in [\"llava-1.5\", \"shikra\"]:\n",
    "    template = SYSTEM_MESSAGE + template\n",
    "question = \"Please help me describe the image in detail.\"\n",
    "\n",
    "# ========== 推理主循环 ==========\n",
    "for batch in tqdm(loader, desc=\"Generating captions\"):\n",
    "    img_ids = batch[\"img_id\"]\n",
    "    images = batch[\"image\"]\n",
    "\n",
    "    questions, kwargs = model_loader.prepare_inputs_for_model(template, [question]*len(img_ids), images)\n",
    "    llama_modify(\n",
    "        model_loader.llm_model,\n",
    "        start_layer,\n",
    "        end_layer,\n",
    "        use_attn,\n",
    "        alpha,\n",
    "        use_cfg,\n",
    "        model_loader.img_start_idx,\n",
    "        model_loader.img_end_idx,\n",
    "    )\n",
    "\n",
    "    logits_processor = (\n",
    "        model_loader.init_cfg_processor(questions, gamma, beam, start_layer, end_layer)\n",
    "        if use_cfg else None\n",
    "    )\n",
    "    if logits_processor is not None:\n",
    "        kwargs[\"logits_processor\"] = LogitsProcessorList([logits_processor])\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model_loader.llm_model.generate(\n",
    "            do_sample=sample,\n",
    "            max_new_tokens=max_tokens,\n",
    "            use_cache=True,\n",
    "            num_beams=beam,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    output_text = model_loader.decode(outputs)\n",
    "\n",
    "    with open(output_jsonl, \"a\", encoding=\"utf-8\") as fout:\n",
    "        for img_id, caption in zip(img_ids, output_text):\n",
    "            fout.write(json.dumps({\"image_id\": int(img_id), \"caption\": caption.strip()}, ensure_ascii=False) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
